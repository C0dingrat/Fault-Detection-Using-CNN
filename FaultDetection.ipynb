{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPZI56xbaS6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abcb919-d2aa-48ad-b597-e91c89384575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# !ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/train\"\n",
        "VAL_DIR   = \"/content/drive/MyDrive/validation\"\n",
        "\n",
        "train_files = sorted([os.path.join(TRAIN_DIR, f) for f in os.listdir(TRAIN_DIR) if f.endswith(\".npz\")])\n",
        "val_files   = sorted([os.path.join(VAL_DIR, f)   for f in os.listdir(VAL_DIR)   if f.endswith(\".npz\")])\n",
        "\n",
        "print(len(train_files), len(val_files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67V8amcjkSR6",
        "outputId": "efa25048-5c6e-460e-ba5c-fcef529422c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SeismicPatchDataset(Dataset):\n",
        "    def __init__(self, files, patch_size=64, fault_ratio=0.5):\n",
        "        self.files = files\n",
        "        self.ps = patch_size\n",
        "        self.fr = fault_ratio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files) * 20\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = self.files[idx % len(self.files)]\n",
        "        data = np.load(file)\n",
        "\n",
        "        seis = data[\"seis\"]\n",
        "        fault = data[\"fault\"]\n",
        "\n",
        "        fault = (fault > 0.5).astype(np.float32)\n",
        "\n",
        "        ps = self.ps\n",
        "        margin = ps // 2\n",
        "\n",
        "        D, H, W = seis.shape # Get actual dimensions of the seismic data\n",
        "\n",
        "        # Calculate safe lower and upper bounds for random center selection\n",
        "        # These bounds ensure that `center - margin` >= 0 and `center + margin <= Dimension`\n",
        "        # If a dimension is too small for a full patch, adjust bounds to prevent ValueError in randint.\n",
        "        lower_bound_d, upper_bound_d = margin, D - margin\n",
        "        if upper_bound_d <= lower_bound_d: # If dimension is too small for a full patch\n",
        "            lower_bound_d = 0\n",
        "            upper_bound_d = D # Allow selection anywhere, patch will be padded later\n",
        "\n",
        "        lower_bound_h, upper_bound_h = margin, H - margin\n",
        "        if upper_bound_h <= lower_bound_h:\n",
        "            lower_bound_h = 0\n",
        "            upper_bound_h = H\n",
        "\n",
        "        lower_bound_w, upper_bound_w = margin, W - margin\n",
        "        if upper_bound_w <= lower_bound_w:\n",
        "            lower_bound_w = 0\n",
        "            upper_bound_w = W\n",
        "\n",
        "        # --- choose patch center ---\n",
        "        cx, cy, cz = 0, 0, 0 # Initialize, will be overwritten\n",
        "\n",
        "        if np.random.rand() < self.fr and fault.sum() > 0:\n",
        "            # fault-biased sampling\n",
        "            fault_idx = np.argwhere(fault == 1)\n",
        "\n",
        "            # Filter fault_idx to only include points from which a full patch can be extracted\n",
        "            valid_fault_idx = []\n",
        "            for fx, fy, fz in fault_idx:\n",
        "                if (fx >= margin and fx < D - margin and\n",
        "                    fy >= margin and fy < H - margin and\n",
        "                    fz >= margin and fz < W - margin):\n",
        "                    valid_fault_idx.append((fx, fy, fz))\n",
        "\n",
        "            if valid_fault_idx:\n",
        "                cx, cy, cz = valid_fault_idx[np.random.randint(len(valid_fault_idx))]\n",
        "            else:\n",
        "                # Fallback to random background if no valid fault_idx within bounds\n",
        "                if upper_bound_d <= lower_bound_d: cx = D // 2\n",
        "                else: cx = np.random.randint(lower_bound_d, upper_bound_d)\n",
        "\n",
        "                if upper_bound_h <= lower_bound_h: cy = H // 2\n",
        "                else: cy = np.random.randint(lower_bound_h, upper_bound_h)\n",
        "\n",
        "                if upper_bound_w <= lower_bound_w: cz = W // 2\n",
        "                else: cz = np.random.randint(lower_bound_w, upper_bound_w)\n",
        "        else:\n",
        "            # random background\n",
        "            if upper_bound_d <= lower_bound_d: cx = D // 2\n",
        "            else: cx = np.random.randint(lower_bound_d, upper_bound_d)\n",
        "\n",
        "            if upper_bound_h <= lower_bound_h: cy = H // 2\n",
        "            else: cy = np.random.randint(lower_bound_h, upper_bound_h)\n",
        "\n",
        "            if upper_bound_w <= lower_bound_w: cz = W // 2\n",
        "            else: cz = np.random.randint(lower_bound_w, upper_bound_w)\n",
        "\n",
        "        # --- extract patch ---\n",
        "        # Get actual slice boundaries. These might result in smaller patches if the image is small\n",
        "        slice_d_start = max(0, cx - margin)\n",
        "        slice_d_end   = min(D, cx + margin)\n",
        "        slice_h_start = max(0, cy - margin)\n",
        "        slice_h_end   = min(H, cy + margin)\n",
        "        slice_w_start = max(0, cz - margin)\n",
        "        slice_w_end   = min(W, cz + margin)\n",
        "\n",
        "        seis_patch = seis[slice_d_start:slice_d_end, slice_h_start:slice_h_end, slice_w_start:slice_w_end]\n",
        "        fault_patch = fault[slice_d_start:slice_d_end, slice_h_start:slice_h_end, slice_w_start:slice_w_end]\n",
        "\n",
        "        # Pad if the extracted patch is smaller than the target patch_size\n",
        "        if seis_patch.shape != (ps, ps, ps):\n",
        "            pad_d = ps - seis_patch.shape[0]\n",
        "            pad_h = ps - seis_patch.shape[1]\n",
        "            pad_w = ps - seis_patch.shape[2]\n",
        "\n",
        "            # Pad with zeros to ensure output patch size is ps x ps x ps\n",
        "            seis_patch = np.pad(seis_patch, ((0, pad_d), (0, pad_h), (0, pad_w)), mode='constant')\n",
        "            fault_patch = np.pad(fault_patch, ((0, pad_d), (0, pad_h), (0, pad_w)), mode='constant')\n",
        "\n",
        "        # --- torch tensors ---\n",
        "        seis_patch = torch.tensor(seis_patch, dtype=torch.float32).unsqueeze(0)\n",
        "        fault_patch = torch.tensor(fault_patch, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return seis_patch, fault_patch"
      ],
      "metadata": {
        "id": "7p9V5mfA1sw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = SeismicPatchDataset(train_files, patch_size=64)\n",
        "val_ds   = SeismicPatchDataset(val_files,   patch_size=64, fault_ratio=0.5)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=1, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "uR3EAPlV2DEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x, y = next(iter(train_loader))\n",
        "# print(x.shape, y.shape)\n",
        "# print(\"Fault voxels:\", y.sum().item())\n"
      ],
      "metadata": {
        "id": "nFXMK44F2QAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "1yNZ94VX2ZtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "KOIyR-My3QzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet3D(nn.Module):\n",
        "    def __init__(self, base_ch=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.enc1 = DoubleConv(1, base_ch)\n",
        "        self.enc2 = DoubleConv(base_ch, base_ch*2)\n",
        "        self.enc3 = DoubleConv(base_ch*2, base_ch*4)\n",
        "        self.enc4 = DoubleConv(base_ch*4, base_ch*8)\n",
        "\n",
        "        self.pool = nn.MaxPool3d(2)\n",
        "\n",
        "        self.bottleneck = DoubleConv(base_ch*8, base_ch*16)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose3d(base_ch*16, base_ch*8, 2, stride=2)\n",
        "        self.dec4 = DoubleConv(base_ch*16, base_ch*8)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose3d(base_ch*8, base_ch*4, 2, stride=2)\n",
        "        self.dec3 = DoubleConv(base_ch*8, base_ch*4)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose3d(base_ch*4, base_ch*2, 2, stride=2)\n",
        "        self.dec2 = DoubleConv(base_ch*4, base_ch*2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose3d(base_ch*2, base_ch, 2, stride=2)\n",
        "        self.dec1 = DoubleConv(base_ch*2, base_ch)\n",
        "\n",
        "        self.out = nn.Conv3d(base_ch, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(self.pool(e1))\n",
        "        e3 = self.enc3(self.pool(e2))\n",
        "        e4 = self.enc4(self.pool(e3))\n",
        "\n",
        "        b = self.bottleneck(self.pool(e4))\n",
        "\n",
        "        d4 = self.up4(b)\n",
        "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
        "\n",
        "        return self.out(d1)\n"
      ],
      "metadata": {
        "id": "HJXfZ2Ch3VRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def dice_loss(pred, target, eps=1e-6):\n",
        "    pred = torch.sigmoid(pred)\n",
        "    num = 2 * (pred * target).sum()\n",
        "    den = pred.sum() + target.sum() + eps\n",
        "    return 1 - num / den\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "    dice = dice_loss(pred, target)\n",
        "    return bce + dice"
      ],
      "metadata": {
        "id": "5M5b_YVT3eLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model = UNet3D(base_ch=16).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "scaler = GradScaler(\"cuda\")\n"
      ],
      "metadata": {
        "id": "dhyj8Etq3XyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(\"cuda\"):\n",
        "            pred = model(x)\n",
        "            loss = combined_loss(pred, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "HgWqJM_S3aFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        pred = model(x)\n",
        "        loss = combined_loss(pred, y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "tnYk81bb3oPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader)\n",
        "    val_loss = validate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl457Nf23rAp",
        "outputId": "d0c7a2d2-6ba0-4d02-d658-4ae4a3c2db77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train: 0.9923 | Val: 0.6356\n",
            "Epoch 002 | Train: 0.4955 | Val: 0.4833\n",
            "Epoch 003 | Train: 0.4200 | Val: 0.4574\n",
            "Epoch 004 | Train: 0.3959 | Val: 0.4498\n",
            "Epoch 005 | Train: 0.3785 | Val: 0.4364\n",
            "Epoch 006 | Train: 0.3650 | Val: 0.4224\n",
            "Epoch 007 | Train: 0.3549 | Val: 0.4293\n",
            "Epoch 008 | Train: 0.3447 | Val: 0.4288\n",
            "Epoch 009 | Train: 0.3389 | Val: 0.4230\n",
            "Epoch 010 | Train: 0.3316 | Val: 0.4123\n",
            "Epoch 011 | Train: 0.3239 | Val: 0.4106\n",
            "Epoch 012 | Train: 0.3203 | Val: 0.4034\n",
            "Epoch 013 | Train: 0.3142 | Val: 0.4214\n",
            "Epoch 014 | Train: 0.3097 | Val: 0.4045\n",
            "Epoch 015 | Train: 0.3039 | Val: 0.3940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = \"/content/drive/MyDrive/unet3d_fault.pth\"\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"epoch\": epoch,\n",
        "}, save_path)\n"
      ],
      "metadata": {
        "id": "naCQT1du3scK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erMfMw1hNh-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}